<!doctype html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link rel="icon" type="image/svg+xml" href="favicon.svg" />

		<title>DEBATE CLUB</title>
		<link rel="stylesheet" href="style.css" />
	</head>

	<a href="../index.html" class="back-link">
		<div class="back-button">
			<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
				<path d="M19 12H5"></path>
				<path d="M12 19l-7-7 7-7"></path>
			</svg>
		</div>
	</a>

	<main>
		<div class="header">
			<h1>DEBATE CLUB</h1>
		</div>
   
	 	<p class="info">So/Se 2025 - Ivo Hartwig </p>
		   
		<section class="Space">
		</section>

		<p class="repo">Team against AGI </p>
		

		<section class="Space">
		</section>

		

		<section class="REFLECTION">
				<div class="text">
					
					
					<h4>Definition of AGI</h4>
					<p>Artificial General Intelligence (AGI) refers to a form of intelligence that matches or surpasses human intellectual capabilities across all domains.
					AGI can self-improve, communicate, and plan ahead.
					It has the potential to outperform humans in most areas.
					Some argue it could be humanity's "last invention."</p>
					
					<h4>Existential Risks</h4>
					<h5>If AGI follows its own objectives, it may act against human interests.</h5>
					<ul>
						<li>Nick Bostrom (Oxford University): The "Alignment Problem" describes systems that are highly competent but not inherently "good."</li>
						<li>Experts such as Stuart Russell, Elon Musk, DeepMind, and OpenAI have warned of existential threats.</li>
						<li>Example: The "Paperclip Maximizer" thought experiment illustrates how a misaligned goal can lead to catastrophic outcomes.</li>
					</ul>
					
					<h4>Lack of Regulation</h4>
					<h5>AGI development is progressing largely without sufficient oversight.</h5>
					<ul>
						<li>The EU's AI Act is a first step, but it is neither globally recognized nor enforceable.</li>
						<li>Whistleblower reports suggest that developers' warnings are often ignored.</li>
						<li>Example: AI systems have already demonstrated deceptive behavior.</li>
					</ul>
					
					<h4>The Alignment Problem</h4>
					<h5>Ensuring that AGI consistently acts in the best interest of humanity remains unsolved.</h5>
					<ul>
						<li>Stuart Russell (UC Berkeley): "There is no method to build an AGI safely."</li>
						<li>Research in this area is underfunded and still at an early stage.</li>
						<li>Example: Current LLMs still hallucinate information.</li>
					</ul>
					
					<h4>Economic Disruption</h4>
					<h5>AGI is likely to cause massive job displacement.</h5>
					<ul>
						<li>Goldman Sachs (2023): Up to 300 million jobs worldwide could be automated.</li>
						<li>OECD: Even highly skilled professions such as legal analysis and software development are at risk.</li>
						<li>Examples: Tools like ChatGPT are already replacing illustrators and developers in some fields.</li>
					</ul>
					
					<h4>Concentration of Power</h4>
					<h5>AGI development is increasingly controlled by a few large corporations.</h5>
					<ul>
						<li>Larger models require billions in resources, making it impossible for smaller companies to compete.</li>
						<li>Data and computing power are centralized in "Big Tech," leading to a potential AI oligarchy.</li>
						<li>Example: OpenAI started as a non-profit but now operates with reduced transparency.</li>
					</ul>
					
					<h4>Misuse</h4>
					<h5>AGI can be exploited by dictatorships, criminals, or other malicious actors.
					</h5>
					<ul>
						<li>In China, AI is already used for facial recognition, social control, and the surveillance of minorities.</li>
						<li>Deepfakes threaten democratic processes through election manipulation and disinformation campaigns.</li>
						<li>Example: The 2024 election saw deepfake videos like "Fake Obama" spreading false information.</li>
					</ul>
					
					<h4>Conclusion</h4>
					<h5>Stronger Oversight is Needed</h5>
					<ul>
						<li>Without control, AGI could be deliberately used to cause harm or consolidate power.</li>
						<li>Social systems cannot keep up with the pace of technological development.</li>
						<li>Prevention is more effective than crisis management
